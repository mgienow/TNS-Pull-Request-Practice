{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmnrnm/TNS-Pull-Request-Practice/blob/master/demo_riffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# riffusion colab demo\n",
        "\n",
        "Run [riffusion](https://www.riffusion.com/about) in a gradio demo with a colab host\n",
        "\n",
        "Riffusion project by [Seth Forsgren](https://twitter.com/sethforsgren) and [Hayk Martiros](https://github.com/hmartiro), colab notebook by [Jasper Gilley](https://twitter.com/0xjasper)\n",
        "\n",
        "Feel free to DM Jasper on Twitter if you have any problems with the notebook\n",
        "\n",
        "Some cool prompt ideas can be found at https://ai-art-wiki.com/wiki/Riffusion#Prompts"
      ],
      "metadata": {
        "id": "HK0XvGHcKeos"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TAeX3URl6ipW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "wtPb1LmTup-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhX8wJzIugkh"
      },
      "outputs": [],
      "source": [
        "#@title Clone the inference repo\n",
        "!git clone https://github.com/riffusion/riffusion.git\n",
        "%cd riffusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -e ."
      ],
      "metadata": {
        "id": "vEpm84ugPuKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from riffusion.streamlit import util"
      ],
      "metadata": {
        "id": "VWkzcQ84QSPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install requirements (you may need to restart the kernel after this)\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "wNrYejGYuyws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import io\n",
        "import numpy as np\n",
        "import dataclasses\n",
        "import IPython.display as ipd\n",
        "from PIL import Image\n",
        "import pydub\n",
        "from random import randint\n",
        "\n",
        "from riffusion.streamlit import util\n",
        "from riffusion.spectrogram_params import SpectrogramParams\n",
        "from riffusion.datatypes import InferenceInput, PromptInput"
      ],
      "metadata": {
        "id": "FwUldklBwbEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = util.load_riffusion_checkpoint(\n",
        "    device=\"cuda\",\n",
        "    checkpoint=util.DEFAULT_CHECKPOINT,\n",
        "    # No trace so we can have variable width\n",
        "    no_traced_unet=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "cJEn1fFHsZN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_audio_from_spectrogram(image):\n",
        "    params = SpectrogramParams(\n",
        "        min_frequency=0,\n",
        "        max_frequency=10000,\n",
        "    )\n",
        "    segment = util.audio_segment_from_spectrogram_image(\n",
        "        image=image,\n",
        "        params=params,\n",
        "        device=\"cuda\",\n",
        "    )\n",
        "    return segment\n",
        "\n",
        "def create_spectrogram_from_prompt(prompt, negative_prompt, guidance=7, seed=42, width=512):\n",
        "    image = util.run_txt2img(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=30,\n",
        "        guidance=7,\n",
        "        negative_prompt=negative_prompt,\n",
        "        seed=seed,\n",
        "        width=width,\n",
        "        height=512,\n",
        "        checkpoint=\"riffusion/riffusion-model-v1\",\n",
        "        device=\"cuda\",\n",
        "        #scheduler=scheduler,\n",
        "    )\n",
        "\n",
        "    ipd.display(image)\n",
        "    \n",
        "    return image\n",
        "\n",
        "\n",
        "  \n",
        "def render(prompt_a, prompt_b, num_interpolation_steps=5, seed_a=None, seeb_b=None, negative_prompt_a=False, negative_prompt_b=False, denoising=.75):\n",
        "    \n",
        "    \"\"\"\n",
        "    Interpolate between prompts in the latent space.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    This tool allows specifying two endpoints and generating a long-form interpolation\n",
        "    between them that traverses the latent space. The interpolation is generated by\n",
        "    the method described at https://www.riffusion.com/about. A seed image is used to\n",
        "    set the beat and tempo of the generated audio, and can be set in the sidebar.\n",
        "    Usually the seed is changed or the prompt, but not both at once. You can browse\n",
        "    infinite variations of the same prompt by changing the seed.\n",
        "    For example, try going from \"church bells\" to \"jazz\" with 10 steps and 0.75 denoising.\n",
        "    This will generate a 50 second clip at 5 seconds per step. Then play with the seeds\n",
        "    or denoising to get different variations.\n",
        "    \"\"\"\n",
        "\n",
        "    #device = \"cuda\"\n",
        "    extension = \"mp3\"\n",
        "\n",
        "    num_inference_steps = 30\n",
        "\n",
        "    guidance = 7 # How much the model listens to the text prompt\n",
        "\n",
        "    init_image_name = \"og_beat\"\n",
        "    \n",
        "\n",
        "    alpha_power = 1\n",
        "\n",
        "\n",
        "    alphas = np.linspace(0, 1, num_interpolation_steps)\n",
        "\n",
        "    # Apply power scaling to alphas to customize the interpolation curve\n",
        "    alphas_shifted = alphas * 2 - 1\n",
        "    alphas_shifted = (np.abs(alphas_shifted) ** alpha_power * np.sign(alphas_shifted) + 1) / 2\n",
        "    alphas = alphas_shifted\n",
        "\n",
        "    if seed_a is None:\n",
        "        seed_a = randint(1,9999)\n",
        "        \n",
        "    if seeb_b is None:\n",
        "        seeb_b = randint(1,9999)\n",
        "\n",
        "    # Prompt inputs A and B in two columns\n",
        "\n",
        "    prompt_input_a = PromptInput(\n",
        "        guidance=guidance, **get_prompt_inputs(prompt_a, negative_prompt_a, seed=seed_a, denoising_default=denoising)\n",
        "    )\n",
        "\n",
        "    prompt_input_b = PromptInput(\n",
        "        guidance=guidance, **get_prompt_inputs(prompt_b, negative_prompt_b, seed=seeb_b, denoising_default=denoising)\n",
        "    )\n",
        "\n",
        "\n",
        "    init_image_path = os.path.join(\"seed_images\", f\"{init_image_name}.png\")\n",
        "    init_image = Image.open(str(init_image_path)).convert(\"RGB\")\n",
        "\n",
        "    # TODO(hayk): Move this code into a shared place and add to riffusion.cli\n",
        "    image_list: T.List[Image.Image] = []\n",
        "    audio_bytes_list: T.List[io.BytesIO] = []\n",
        "    for i, alpha in enumerate(alphas):\n",
        "        inputs = InferenceInput(\n",
        "            alpha=float(alpha),\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            seed_image_id=\"og_beat\",\n",
        "            start=prompt_input_a,\n",
        "            end=prompt_input_b,\n",
        "        )\n",
        "\n",
        "        if i == 0:\n",
        "            print(\"Example input JSON\")\n",
        "            print(dataclasses.asdict(inputs))\n",
        "\n",
        "        image, audio_bytes = run_interpolation(\n",
        "            pipeline,\n",
        "            inputs=inputs,\n",
        "            init_image=init_image,\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "        image_list.append(image)\n",
        "        audio_bytes_list.append(audio_bytes)\n",
        "\n",
        "    # TODO(hayk): Concatenate with overlap and better blending like in audio to audio\n",
        "    audio_segments = [pydub.AudioSegment.from_file(audio_bytes) for audio_bytes in audio_bytes_list]\n",
        "    concat_segment = audio_segments[0]\n",
        "    for segment in audio_segments[1:]:\n",
        "        concat_segment = concat_segment.append(segment, crossfade=0)\n",
        "\n",
        "    audio_bytes = io.BytesIO()\n",
        "    concat_segment.export(audio_bytes, format=extension)\n",
        "    audio_bytes.seek(0)\n",
        "\n",
        "    print(f\"Duration: {concat_segment.duration_seconds:.3f} seconds\")\n",
        "    #ipd.display(ipd.Audio(audio_bytes))\n",
        "\n",
        "    output_name = (\n",
        "        f\"{prompt_input_a.prompt.replace(' ', '_')}_\"\n",
        "        f\"{prompt_input_b.prompt.replace(' ', '_')}.{extension}\"\n",
        "    )\n",
        "    return audio_bytes, concat_segment\n",
        "    \n",
        "\n",
        "def get_prompt_inputs(\n",
        "    prompt,\n",
        "    negative_prompt,\n",
        "    seed,\n",
        "    denoising_default: float = 0.5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute prompt inputs from widgets.\n",
        "    \"\"\"\n",
        "    p = {}\n",
        "\n",
        "    p[\"prompt\"] = prompt\n",
        "\n",
        "    if negative_prompt:\n",
        "        p[\"negative_prompt\"] = negative_prompt\n",
        "\n",
        "    p[\"seed\"] = seed\n",
        "\n",
        "    p[\"denoising\"] = denoising_default\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "def run_interpolation(\n",
        "    pipeline,\n",
        "    inputs: InferenceInput,\n",
        "    init_image: Image.Image,\n",
        "    extension: str = \"mp3\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Cached function for riffusion interpolation.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    image = pipeline.riffuse(\n",
        "        inputs,\n",
        "        init_image=init_image,\n",
        "        mask_image=None,\n",
        "    )\n",
        "\n",
        "    # TODO(hayk): Change the frequency range to [20, 20k] once the model is retrained\n",
        "    params = SpectrogramParams(\n",
        "        min_frequency=0,\n",
        "        max_frequency=10000,\n",
        "    )\n",
        "\n",
        "    # Reconstruct from image to audio\n",
        "    audio_bytes = util.audio_bytes_from_spectrogram_image(\n",
        "        image=image,\n",
        "        params=params,\n",
        "        device=\"cuda\",\n",
        "        output_format=extension,\n",
        "    )\n",
        "\n",
        "    return image, audio_bytes"
      ],
      "metadata": {
        "id": "nwHJlDcaPdSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run with Colab interface\n",
        "\n",
        "prompt= 'dog barking'#@param {type:\"string\"}\n",
        "negative_prompt = \"\"#@param {type:\"string\"}\n",
        "\n",
        "\n",
        "image = create_spectrogram_from_prompt(prompt, negative_prompt, guidance=20)\n",
        "\n",
        "make_audio_from_spectrogram(image)"
      ],
      "metadata": {
        "id": "uO7oM1UGsL5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_bytes, concat_segment = render('piano', 'oboe', num_interpolation_steps=5)"
      ],
      "metadata": {
        "id": "hgiOk_MORq4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concat_segment"
      ],
      "metadata": {
        "id": "TOu98mjoRwXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "42elsn75R6Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x311CdkQR-g1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}